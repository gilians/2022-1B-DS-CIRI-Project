{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e76db8f2-e90d-4ad5-9fdc-0e850d3c8fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import tqdm\n",
    "import os\n",
    "import re\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb296568-3ca8-4dfc-80bd-2f758ec71899",
   "metadata": {},
   "source": [
    "Set the device. Using CUDA on CUDA-enabled devices speeds up the use of convolutional networks significantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efd0421d-267f-4b2a-aebc-4fe56ac71c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda_enabled = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if cuda_enabled else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e59d07-8d93-40e6-a592-2ead337d4641",
   "metadata": {},
   "source": [
    "We create the descriptors through transfer learning. We remove the last fully-connected (classification) layers of pre-trained models and use the output of the convolutional part of the respective model as descriptors. We do this for:\n",
    "\n",
    "- VGG16\n",
    "\n",
    "These models were chosen because of ... **TODO: List reasons for choosing models**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5cdc87-ce3c-4282-baa1-126698822d47",
   "metadata": {},
   "source": [
    "## Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77af039b-7da4-4409-bc95-30f85c87d4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because the dataset contains images with a filename starting with a \".\",\n",
    "# these files are hidden, giving problems down the line with reading. As such,\n",
    "# rename them by adding \"img\" as a filename prefix.\n",
    "\n",
    "base_path = \"./data/Incidents-subset\"\n",
    "directories = os.listdir(os.path.expanduser(base_path))\n",
    "for directory in directories:\n",
    "    files = os.listdir(os.path.expanduser(base_path + \"/\" + directory))\n",
    "    for file in files:\n",
    "        if re.match(r\".*.((jpg)|(png)|(jpeg))\", file, re.IGNORECASE) and not re.match(r\"img.*\", file):\n",
    "            os.rename(os.path.expanduser(base_path + \"/\" + directory + \"/\" + file), os.path.expanduser(base_path + \"/\" + directory + \"/img\" + file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77a722cb-8800-41e4-a074-60293e90e916",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some images are corrupted, so we need a check whether this is the case\n",
    "# before images are added to the dataset. This is done by calling the function\n",
    "# below in the is_valid_file parameter of the ImageFolder function.\n",
    "\n",
    "def check_Image(path):\n",
    "    try:\n",
    "        im = Image.open(path)\n",
    "        return True\n",
    "    except:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4256b930-da86-452d-bad5-070d2af5da78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the same transform and normalisation as used in the paper source code\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                    std=[0.229, 0.224, 0.225])\n",
    "\n",
    "transform = transforms.Compose([\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ])\n",
    "\n",
    "dataset = torchvision.datasets.ImageFolder('./data/Incidents-subset', transform=transform, is_valid_file=check_Image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "797aa699-6e78-43eb-834b-03e92074a1b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/2022-1B-DS-CIRI-Project/env/lib/python3.10/site-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "n_classes = 12\n",
    "\n",
    "data_train, data_test = torch.utils.data.random_split(dataset, [0.7, 0.3])\n",
    "\n",
    "targets_train = [target for (_, target) in data_train]\n",
    "targets_test = [target for (_, target) in data_test]\n",
    "\n",
    "training_dl = DataLoader(data_train, batch_size=4, shuffle=False)\n",
    "testing_dl = DataLoader(data_test, batch_size=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b690f4-81ea-4dba-a7c3-e93b84cf3e9a",
   "metadata": {},
   "source": [
    "## Visualise datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7d3e30-83a5-48f7-a2b3-db5986065c8b",
   "metadata": {},
   "source": [
    "## Initialize models\n",
    "\n",
    "### VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1beeee9-c082-4f38-999e-aff07ceeb244",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vgg16 = torchvision.models.vgg16(weights=\"DEFAULT\")\n",
    "\n",
    "for param in model_vgg16.parameters():\n",
    "    # As the model will not be trained, gradients are not required. Disabling\n",
    "    # them speeds up performance.\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Set model to evaluation mode for reasons\n",
    "# TODO: Check why model should be set to evaluation mode\n",
    "model_vgg16.eval()\n",
    "\n",
    "# Empty CUDA cache to prevent memory issues\n",
    "if cuda_enabled:\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Replace VGG16 classifier with identity layer to allow descriptor extraction\n",
    "model_vgg16.classifier = nn.Identity()\n",
    "\n",
    "# Move model to previously set device, speeding up performance if CUDA-enabled\n",
    "model_vgg16 = model_vgg16.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719e8a9c-b5ff-47fd-8ec2-724dd95a8288",
   "metadata": {},
   "source": [
    "## Baseline\n",
    "\n",
    "### Obtain image descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "10d78568-4e8d-4331-a6d5-b1f335a49d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_descriptors_from_model(model, dataloader):\n",
    "    \"\"\"Returns the output of the provided model for all items in the dataloader.\"\"\"\n",
    "    outputs = []\n",
    "\n",
    "    # Improve performance by disabling unnecessary gradient calculation\n",
    "    with torch.no_grad():\n",
    "        for data, targets in tqdm.tqdm(dataloader):\n",
    "            data = data.to(device)\n",
    "            output = model(data).detach()\n",
    "            outputs.extend(output)\n",
    "\n",
    "    return outputs\n",
    "\n",
    "\n",
    "def transform_descriptors_to_numpy(descriptors):\n",
    "    \"\"\"Returns a numpy array derived from a provided list of tensors.\"\"\"\n",
    "    return np.array([descriptor.cpu().numpy() for descriptor in descriptors])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce5261d8-864d-4447-b764-66d9eec1759e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1288/1288 [02:21<00:00,  9.12it/s]\n",
      "100%|██████████| 552/552 [00:52<00:00, 10.50it/s]\n"
     ]
    }
   ],
   "source": [
    "descriptors_train = get_descriptors_from_model(model_vgg16, training_dl)\n",
    "descriptors_test = get_descriptors_from_model(model_vgg16, testing_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "98ab57a2-b584-4737-a1f1-e6a0748b4108",
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptors_train = transform_descriptors_to_numpy(descriptors_train)\n",
    "descriptors_test = transform_descriptors_to_numpy(descriptors_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d79ee2b-33b2-4974-ac50-950fe278574c",
   "metadata": {},
   "source": [
    "### Perform k-nearest neighbors (kNN) classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29855874-74f0-46be-81f8-8565f6a85305",
   "metadata": {},
   "source": [
    "First, we perform the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c9351e18-26ce-42fe-825a-49fd820ef082",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_knn_classifiers_for_neighbors(n_neighbors_list, data, targets):\n",
    "    classifiers = {}\n",
    "    for n_neighbors in n_neighbors_list:\n",
    "        classifier = KNeighborsClassifier(n_neighbors=n_neighbors).fit(data,\n",
    "                                                                       targets)\n",
    "        classifiers[n_neighbors] = classifier\n",
    "    return classifiers\n",
    "\n",
    "\n",
    "def get_knn_classifiers_predictions(knn_classifiers, data):\n",
    "    predictions = []\n",
    "    for classifier in tqdm.tqdm(knn_classifiers):\n",
    "        predictions.append(knn_classifiers[classifier].predict(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364a617b-36fd-4f1e-87b2-b7cf6df1ebae",
   "metadata": {},
   "source": [
    "First, we create KNN classifiers for all numbers of neighbors we would like to assess."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "043f3b66-e29a-4b08-a633-f61452a18750",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neighbors_list = [2, 5, 10, 15]\n",
    "classifiers = get_knn_classifiers_for_neighbors(n_neighbors_list,\n",
    "                                                descriptors_train,\n",
    "                                                targets_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb5f8a0-ad98-4be9-a9ce-5cb217241edd",
   "metadata": {},
   "source": [
    "For each of the previously created kNN classifiers, we obtain the predictions for both the training data and the test data. This allows an assessment of the performance of the classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d01f18d6-854f-4dd6-a136-1a96e1dc1811",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:24<00:00,  6.18s/it]\n",
      "100%|██████████| 4/4 [00:15<00:00,  3.87s/it]\n"
     ]
    }
   ],
   "source": [
    "predictions_train_list = get_knn_classifiers_predictions(classifiers, descriptors_train)\n",
    "predictions_test_list = get_knn_classifiers_predictions(classifiers, descriptors_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043e854e-af71-4d96-acb9-7488c18aff1e",
   "metadata": {},
   "source": [
    "We now assess the accuracy of the predictions of each classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c2620f4a-313a-442d-8f7f-a7fa2ca561c9",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m train_accuracies_per_n_neighbors \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, predictions \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpredictions_train_list\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m      3\u001b[0m     accuracy \u001b[38;5;241m=\u001b[39m accuracy_score(targets_train, predictions)\n\u001b[1;32m      4\u001b[0m     train_accuracies_per_n_neighbors\u001b[38;5;241m.\u001b[39mappend((n_neighbors_list[i], accuracy))\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "train_accuracies_per_n_neighbors = []\n",
    "for i, predictions in enumerate(predictions_train_list):\n",
    "    accuracy = accuracy_score(targets_train, predictions)\n",
    "    train_accuracies_per_n_neighbors.append((n_neighbors_list[i], accuracy))\n",
    "\n",
    "test_accuracies_per_n_neighbors = {}\n",
    "for i, predictions in enumerate(predictions_test_list):\n",
    "    accuracy = accuracy_score(targets_train, predictions)\n",
    "    test_accuracies_per_n_neighbors.append((n_neighbors_list[i], accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9221e5-6d6f-4e89-8906-5ac61f8bdefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_training = PCA(n_components=2).fit_transform(descriptors_train)\n",
    "pca_testing = PCA(n_components=2).fit_transform(descriptors_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ce636b-2391-4089-894f-fefc16d53430",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions_against_targets(data,\n",
    "                                     predictions,\n",
    "                                     targets,\n",
    "                                     n_classes,\n",
    "                                     cmap=None):\n",
    "    \"\"\"Draws a scatter plot of two-dimensional data which highlights\n",
    "    differences between targets and predictions through distinct edge and fill\n",
    "    colors.\"\"\"\n",
    "\n",
    "    if (cmap is None):\n",
    "        cmap = plt.get_cmap(\"rainbow\")\n",
    "\n",
    "    colors_target = cmap(targets / n_classes)\n",
    "    colors_predictions = cmap(predictions / n_classes)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    # Plot targets (ground truth) with fill color representing target class\n",
    "    ax.scatter(data[:, 0],\n",
    "               data[:, 1],\n",
    "               facecolors=colors_target)\n",
    "    \n",
    "    # Plot predictions with edge color representing predicted class\n",
    "    ax.scatter(data[:, 0],\n",
    "               data[:, 1],\n",
    "               edgecolors=colors_predictions)\n",
    "\n",
    "    return (fig, ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3ca379-ae94-490e-b441-2ad91369dd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax_train_scatter = plot_predictions_against_targets(pca_training,\n",
    "                                                       best_predictions_train,\n",
    "                                                       targets_train,\n",
    "                                                       n_classes)\n",
    "ax_train_scatter.set_title(\"Two-component PCA, training data\")\n",
    "\n",
    "_, ax_test_scatter = plot_predictions_against_targets(pca_testing,\n",
    "                                                      best_predictions_test,\n",
    "                                                      targets_test,\n",
    "                                                      n_classes)\n",
    "ax_test_scatter.set_title(\"Two-component PCA, test data\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CIRI-env",
   "language": "python",
   "name": "ciri-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
