{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0d519fd-0dfa-473a-9747-9a209ac62b0c",
   "metadata": {},
   "source": [
    "# Data Science project: CIRI\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e76db8f2-e90d-4ad5-9fdc-0e850d3c8fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import tqdm\n",
    "import os\n",
    "import re\n",
    "\n",
    "from enum import Enum\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eced99f0-ad83-4613-8880-0b2cb63bf1c9",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80f4a2f1-02bb-49ac-8b29-d5ddc8beed34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Whether model descriptors should be extracted for the baseline (True), or\n",
    "# loaded from the filesystem (False)\n",
    "extract_model_descriptors = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2835e204-e1a4-4738-9103-84074c7b97d3",
   "metadata": {},
   "source": [
    "## Generic setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb296568-3ca8-4dfc-80bd-2f758ec71899",
   "metadata": {},
   "source": [
    "Set the device. Using CUDA on CUDA-enabled devices speeds up the use of convolutional networks significantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efd0421d-267f-4b2a-aebc-4fe56ac71c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda_enabled = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if cuda_enabled else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8900377-950c-43e4-a2c4-85cec53a834c",
   "metadata": {},
   "source": [
    "We will be using a random number generator every now and then throughout this notebook. By initializing it here, we can change a single seed to obtain the same/different results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c3d7889-252f-409a-873f-3edd809f136d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e59d07-8d93-40e6-a592-2ead337d4641",
   "metadata": {},
   "source": [
    "We create the descriptors through transfer learning. We remove the last fully-connected (classification) layers of pre-trained models and use the output of the convolutional part of the respective model as descriptors. We do this for:\n",
    "\n",
    "- VGG16\n",
    "\n",
    "These models were chosen because of ... **TODO: List reasons for choosing models**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5cdc87-ce3c-4282-baa1-126698822d47",
   "metadata": {},
   "source": [
    "## Load datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa269c6-7168-492b-a441-389f968ebd60",
   "metadata": {},
   "source": [
    "Because the dataset contains images with a filename starting with a dot (\".\"), these files are hidden, giving problems down the line with reading. As such, rename them by adding \"img\" as a filename prefix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77af039b-7da4-4409-bc95-30f85c87d4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"./data/Incidents-subset\"\n",
    "directories = os.listdir(os.path.expanduser(base_path))\n",
    "for directory in directories:\n",
    "    files = os.listdir(os.path.expanduser(base_path + \"/\" + directory))\n",
    "    for file in files:\n",
    "        if re.match(r\".*.((jpg)|(png)|(jpeg))\", file, re.IGNORECASE) and not re.match(r\"img.*\", file):\n",
    "            os.rename(os.path.expanduser(base_path + \"/\" + directory + \"/\" + file), os.path.expanduser(base_path + \"/\" + directory + \"/img\" + file))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6afb278-ea1e-4767-9da0-483e0e58fc10",
   "metadata": {},
   "source": [
    "Some images are corrupted, so we need a check whether this is the case before images are added to the dataset. This is done by calling the function below in the `is_valid_file` parameter of the `ImageFolder` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77a722cb-8800-41e4-a074-60293e90e916",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_Image(path):\n",
    "    try:\n",
    "        im = Image.open(path)\n",
    "        return True\n",
    "    except:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3224870d-eb4a-425e-be84-7a5ef2c02c68",
   "metadata": {},
   "source": [
    "For transformation and normalization, we use the same parameter as used in the paper source code.\n",
    "\n",
    "> Weber, E., Papadopoulos, D.P., Lapedriza, A., Ofli, F., Imran, M. and Torralba, A. 2022. Incidents1M: a large-scale dataset of images with natural disasters, damage, and incidents. arXiv.\n",
    "\n",
    "Source code: https://github.com/ethanweber/IncidentsDataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4256b930-da86-452d-bad5-070d2af5da78",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Found no valid file for the classes VGG16. ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 13\u001b[0m\n\u001b[1;32m      1\u001b[0m normalize \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mNormalize(\n\u001b[1;32m      2\u001b[0m     mean\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.485\u001b[39m, \u001b[38;5;241m0.456\u001b[39m, \u001b[38;5;241m0.406\u001b[39m],\n\u001b[1;32m      3\u001b[0m     std\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.229\u001b[39m, \u001b[38;5;241m0.224\u001b[39m, \u001b[38;5;241m0.225\u001b[39m]\n\u001b[1;32m      4\u001b[0m )\n\u001b[1;32m      6\u001b[0m transform \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[1;32m      7\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mResize(\u001b[38;5;241m256\u001b[39m),\n\u001b[1;32m      8\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mCenterCrop(\u001b[38;5;241m224\u001b[39m),\n\u001b[1;32m      9\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mToTensor(),\n\u001b[1;32m     10\u001b[0m     normalize,\n\u001b[1;32m     11\u001b[0m ])\n\u001b[0;32m---> 13\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mtorchvision\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdatasets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mImageFolder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./data/Incidents-subset\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_valid_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_Image\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/2022-1B-DS-CIRI-Project/env/lib/python3.10/site-packages/torchvision/datasets/folder.py:309\u001b[0m, in \u001b[0;36mImageFolder.__init__\u001b[0;34m(self, root, transform, target_transform, loader, is_valid_file)\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    302\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    303\u001b[0m     root: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    307\u001b[0m     is_valid_file: Optional[Callable[[\u001b[38;5;28mstr\u001b[39m], \u001b[38;5;28mbool\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    308\u001b[0m ):\n\u001b[0;32m--> 309\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m        \u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mIMG_EXTENSIONS\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mis_valid_file\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget_transform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_transform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_valid_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_valid_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamples\n",
      "File \u001b[0;32m~/2022-1B-DS-CIRI-Project/env/lib/python3.10/site-packages/torchvision/datasets/folder.py:145\u001b[0m, in \u001b[0;36mDatasetFolder.__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform, is_valid_file)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(root, transform\u001b[38;5;241m=\u001b[39mtransform, target_transform\u001b[38;5;241m=\u001b[39mtarget_transform)\n\u001b[1;32m    144\u001b[0m classes, class_to_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfind_classes(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot)\n\u001b[0;32m--> 145\u001b[0m samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_to_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextensions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_valid_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader \u001b[38;5;241m=\u001b[39m loader\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextensions \u001b[38;5;241m=\u001b[39m extensions\n",
      "File \u001b[0;32m~/2022-1B-DS-CIRI-Project/env/lib/python3.10/site-packages/torchvision/datasets/folder.py:189\u001b[0m, in \u001b[0;36mDatasetFolder.make_dataset\u001b[0;34m(directory, class_to_idx, extensions, is_valid_file)\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m class_to_idx \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    185\u001b[0m     \u001b[38;5;66;03m# prevent potential bug since make_dataset() would use the class_to_idx logic of the\u001b[39;00m\n\u001b[1;32m    186\u001b[0m     \u001b[38;5;66;03m# find_classes() function, instead of using that of the find_classes() method, which\u001b[39;00m\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;66;03m# is potentially overridden and thus could have a different logic.\u001b[39;00m\n\u001b[1;32m    188\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe class_to_idx parameter cannot be None.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 189\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmake_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_to_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextensions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextensions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_valid_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_valid_file\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/2022-1B-DS-CIRI-Project/env/lib/python3.10/site-packages/torchvision/datasets/folder.py:102\u001b[0m, in \u001b[0;36mmake_dataset\u001b[0;34m(directory, class_to_idx, extensions, is_valid_file)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m extensions \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    101\u001b[0m         msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSupported extensions are: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mextensions\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mif\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28misinstance\u001b[39m(extensions,\u001b[38;5;250m \u001b[39m\u001b[38;5;28mstr\u001b[39m)\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01melse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(extensions)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 102\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(msg)\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m instances\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: Found no valid file for the classes VGG16. "
     ]
    }
   ],
   "source": [
    "normalize = transforms.Normalize(\n",
    "    mean=[0.485, 0.456, 0.406],\n",
    "    std=[0.229, 0.224, 0.225]\n",
    ")\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    normalize,\n",
    "])\n",
    "\n",
    "data = torchvision.datasets.ImageFolder(\n",
    "    './data/Incidents-subset',\n",
    "    transform=transform,\n",
    "    is_valid_file=check_Image\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a90880-901f-4d07-a2e8-bac0e3be9380",
   "metadata": {},
   "source": [
    "Next, we split the data into a train and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797aa699-6e78-43eb-834b-03e92074a1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataType(Enum):\n",
    "    TRAIN = \"train\"\n",
    "    TEST = \"test\"\n",
    "\n",
    "\n",
    "class Dataset:\n",
    "    def __init__(self, data, ratios):\n",
    "        self.all_data = data\n",
    "        self.all_targets = self._get_targets(data)\n",
    "        self.all_classes = data.classes\n",
    "\n",
    "        data_splits = torch.utils.data.random_split(data, list(ratios.values()))\n",
    "        labels = ratios.keys()\n",
    "\n",
    "        self.data = {}\n",
    "        for i, label in enumerate(labels):\n",
    "            self.data[label] = data_splits[i]\n",
    "\n",
    "        self.targets = {}\n",
    "        for label, data in self.data.items():\n",
    "            self.targets[label] = self._get_targets(data)\n",
    "\n",
    "        self.dataloaders = {}\n",
    "        for label, data in self.data.items():\n",
    "            self.dataloaders[label] = DataLoader(data, batch_size=4, shuffle=False)\n",
    "\n",
    "    def _get_targets(self, data):\n",
    "        return [target for (_, target) in data]\n",
    "\n",
    "\n",
    "dataset = Dataset(data, {DataType.TRAIN: 0.7, DataType.TEST: 0.3})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0728bd7c-a6e1-4cfe-bd67-806355d23985",
   "metadata": {},
   "source": [
    "## Visualise datasets\n",
    "\n",
    "### General information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac8fc30-b68c-493e-bd0c-7b12ca9596e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of items in dataset: {len(dataset.all_data)}\")\n",
    "print(f\"Number of items in train set: {len(dataset.data[DataType.TRAIN])}\")\n",
    "print(f\"Number of images in test set: {len(dataset.data[DataType.TEST])}\")\n",
    "print(f\"Number of classes: {len(dataset.all_classes)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6c53b0-076d-413d-901b-339aebf14827",
   "metadata": {},
   "source": [
    "### Class distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29dae6d8-04c8-472f-8df0-95b76e0063f7",
   "metadata": {},
   "source": [
    "First, we assess the number of images per class in both the train and test set. This provides us with a rough idea of the distribution in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ec6276-8f64-4492-849b-200513c14d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_targets, unique_target_counts = np.unique(\n",
    "    dataset.all_targets,\n",
    "    return_counts=True\n",
    ")\n",
    "unique_targets_train, unique_target_counts_train = np.unique(\n",
    "    dataset.targets[DataType.TRAIN],\n",
    "    return_counts=True\n",
    ")\n",
    "unique_targets_test, unique_target_counts_test = np.unique(\n",
    "    dataset.targets[DataType.TEST],\n",
    "    return_counts=True\n",
    ")\n",
    "\n",
    "# Assert that train and test set both contain the same number of classes, as subsequent code relies on this assumption\n",
    "assert len(unique_targets_train) == len(dataset.all_classes), \"Train set must contain all classes\"\n",
    "assert len(unique_targets_test) == len(dataset.all_classes), \"Test set must contain all classes\"\n",
    "\n",
    "# Convoluted way of showing a bar chart with multiple bars, but prevents having to create a dataframe for e.g. Seaborn\n",
    "plt.bar(np.arange(len(dataset.all_classes)) - 0.25,\n",
    "        unique_target_counts,\n",
    "        width=0.25,\n",
    "        label=\"all\",\n",
    "        color=\"black\")\n",
    "plt.bar(np.arange(len(dataset.all_classes)),\n",
    "        unique_target_counts_train,\n",
    "        width=0.25,\n",
    "        # color=\"none\",\n",
    "        # edgecolor=\"black\",\n",
    "        # hatch=\"----\",\n",
    "        label=\"train\")\n",
    "plt.bar(np.arange(len(dataset.all_classes)) + 0.25,\n",
    "        unique_target_counts_test,\n",
    "        width=0.25,\n",
    "        # color=\"none\",\n",
    "        # edgecolor=\"black\",\n",
    "        label=\"test\")\n",
    "plt.xticks(np.arange(len(dataset.all_classes)),\n",
    "           labels=dataset.all_classes,\n",
    "           rotation=\"vertical\")\n",
    "plt.legend()\n",
    "plt.title(\"Number of images per class\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc71164c-c33d-4ddd-b6cd-e0a4e4307302",
   "metadata": {},
   "source": [
    "The plot shows that the data in the dataset is unbalanced. For example, the \"car accident\" class sees a much larger occurence than the \"nuclear explosion\" class. This could lead to issues later on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60fe60c-f52d-47b5-b0e1-79688f200c5c",
   "metadata": {},
   "source": [
    "### Sample images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5543ec0-bfeb-4734-a6d1-00af4cd3e33a",
   "metadata": {},
   "source": [
    "As a result of the normalization performed earlier, the images cannot be displayed as-is without clipping. Hence, we need a way to inverse the normalization before displaying the images. We do this through the use of a `Transform`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a68164-9086-4e07-b7f9-ca15bc62d084",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalizeInverse(torchvision.transforms.Normalize):\n",
    "    \"\"\"\n",
    "    Undoes the normalization and returns the reconstructed images in the input domain.\n",
    "    Code from https://discuss.pytorch.org/t/simple-way-to-inverse-transform-normalization/4821/8\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mean, std):\n",
    "        mean = torch.as_tensor(mean)\n",
    "        std = torch.as_tensor(std)\n",
    "        std_inv = 1 / (std + 1e-7)\n",
    "        mean_inv = -mean * std_inv\n",
    "        super().__init__(mean=mean_inv, std=std_inv)\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        return super().__call__(tensor.clone())\n",
    "\n",
    "\n",
    "inverse_normalize = NormalizeInverse(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb9752f-032b-4646-b50b-ad2658a47306",
   "metadata": {},
   "source": [
    "Now, we actually visualise the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313398c8-c297-4b7d-9569-6ce01cce4639",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_images_per_class = 4\n",
    "\n",
    "# Create plots with sufficient size and spacing\n",
    "fig, axs = plt.subplots(len(dataset.all_classes),\n",
    "                        number_of_images_per_class,\n",
    "                        figsize=(10, 20))\n",
    "plt.subplots_adjust(hspace=.5, wspace=.5)\n",
    "\n",
    "for target_class in range(0, len(dataset.all_classes)):\n",
    "    # Create a mask for the dataset to include only images for this class\n",
    "    relevant_image_indices = (np.array(dataset.all_targets) == target_class).nonzero()[0]\n",
    "\n",
    "    # Randomly sample images from all relevant images\n",
    "    sampled_image_indices = rng.choice(\n",
    "        a=relevant_image_indices,\n",
    "        size=number_of_images_per_class,\n",
    "        replace=False\n",
    "    )\n",
    "\n",
    "    # Display the images with the proper title\n",
    "    for i, image_index in enumerate(sampled_image_indices):\n",
    "        ax = axs[target_class, i]\n",
    "        ax.imshow(\n",
    "            # Clamp to prevent values just outside the [0,1] range (as a result of inverse normalisation)\n",
    "            torch.clamp(inverse_normalize(dataset.all_data[image_index][0]).permute(1,2,0), 0, 1),\n",
    "            vmin=0,\n",
    "            vmax=1)\n",
    "        ax.axis(\"off\")\n",
    "        ax.set_title(dataset.all_classes[target_class])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283441ef-cf5f-431a-aa00-3ebb6bca34f9",
   "metadata": {},
   "source": [
    "## Initialize models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ca0ab6-ab25-4277-b8dd-3a75a4423901",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model():\n",
    "    def __init__(self, model, name):\n",
    "        self.model = model\n",
    "        self.name = name\n",
    "\n",
    "        for param in self.model.parameters():\n",
    "            # As the model will not be trained, gradients are not required.\n",
    "            # Disabling them speeds up performance.\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.model.eval()\n",
    "        self.remove_classifier()\n",
    "\n",
    "    def remove_classifier(self):\n",
    "        \"\"\"\n",
    "        Removes the last fully connected layer (the classifier) from the\n",
    "        model, allowing the extraction of descriptors.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def to(self, device):\n",
    "        self.model = self.model.to(device)\n",
    "\n",
    "    def descriptors(self, dataloader, as_numpy=True):\n",
    "        \"\"\"\n",
    "        Returns the output of the model for all items in the dataloader.\n",
    "        \"\"\"\n",
    "        outputs = []\n",
    "        with torch.no_grad():\n",
    "            for data, targets in tqdm.tqdm(dataloader):\n",
    "                data = data.to(device)\n",
    "                output = self.model(data).detach()\n",
    "                outputs.extend(output)\n",
    "\n",
    "        if as_numpy:\n",
    "            outputs = np.array([output.cpu().numpy() for output in outputs])\n",
    "\n",
    "        return outputs\n",
    "\n",
    "\n",
    "models = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0982321e-151f-43a8-a548-5355ea705d8f",
   "metadata": {},
   "source": [
    "### VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a31661-d35c-496c-9f87-d1d64f3fc372",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vgg16Model(Model):\n",
    "    def __init__(self):\n",
    "        model = torchvision.models.vgg16(weights=\"DEFAULT\")\n",
    "        super().__init__(model, \"VGG16\")\n",
    "\n",
    "    def remove_classifier(self):\n",
    "        self.model.classifier = nn.Identity()\n",
    "\n",
    "\n",
    "vgg16 = Vgg16Model()\n",
    "vgg16.to(device)\n",
    "\n",
    "models.append(vgg16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40927677-2806-4313-a94c-8b7370da4ac4",
   "metadata": {},
   "source": [
    "### Inception V3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1beeee9-c082-4f38-999e-aff07ceeb244",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InceptionV3Model(Model):\n",
    "    def __init__(self):\n",
    "        model = torchvision.models.inception_v3(weights=\"DEFAULT\")\n",
    "        super().__init__(model, \"InceptionV3\")\n",
    "\n",
    "    def remove_classifier(self):\n",
    "        self.model.fc = nn.Identity()\n",
    "\n",
    "\n",
    "inception = InceptionV3Model()\n",
    "inception.to(device)\n",
    "\n",
    "models.append(inception)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719e8a9c-b5ff-47fd-8ec2-724dd95a8288",
   "metadata": {},
   "source": [
    "## Baseline\n",
    "\n",
    "### Obtain image descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7ef902-5160-428d-942d-16bacadd1482",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DescriptorManager:\n",
    "    \"\"\"\n",
    "    Handles the loading, saving and extraction of data descriptors from a model...\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, base_path=\"data/\", extract=False):\n",
    "        self.base_path = base_path\n",
    "        self.extract = extract\n",
    "\n",
    "    def create_path(self, model_name, label):\n",
    "        \"\"\"\n",
    "        Creates a standardised path for descriptors with the provided model name\n",
    "        and label to prevent inconsistencies between loading and saving descriptors.\n",
    "        \"\"\"\n",
    "        return f\"{base_path}/{model_name}/descriptors_{label}.bin\"\n",
    "\n",
    "    def safe_open_wb(self, path):\n",
    "        \"\"\"\n",
    "        Open \"path\" for writing, creating any parent directories as needed.\n",
    "        \"\"\"\n",
    "        os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "\n",
    "        return open(path, 'wb')\n",
    "\n",
    "    def save_descriptors(self, descriptors, model_name, label):\n",
    "        \"\"\"\n",
    "        Saves the provided descriptor to the file system at a path derived from\n",
    "        the model name and label.\n",
    "        \"\"\"\n",
    "        path = self.create_path(model_name, label)\n",
    "        with self.safe_open_wb(path) as file:\n",
    "            pickle.dump(descriptors, file)\n",
    "\n",
    "    def extract_save_descriptors(self, dataloader, model, label):\n",
    "        \"\"\"\n",
    "        Extracts descriptors from the provided model and saves them to the\n",
    "        file system.\n",
    "        \"\"\"\n",
    "        descriptors = model.descriptors(dataloader, True)\n",
    "        self.save_descriptors(descriptors, model.name, label)\n",
    "\n",
    "        return descriptors\n",
    "\n",
    "    def load_descriptors(self, model_name, label):\n",
    "        \"\"\"\n",
    "        Loads and returns the provided descriptors from the file system, based\n",
    "        on the provided model name and label.\n",
    "        \"\"\"\n",
    "        path = self.create_path(model_name, label)\n",
    "        with open(path, \"rb\") as file:\n",
    "            return pickle.load(file)\n",
    "\n",
    "    def get_descriptors(self, dataloader, model, label):\n",
    "        \"\"\"\n",
    "        Returns descriptors from the file system or extracts them from the model,\n",
    "        depending on the DescriptorManager \"extract\" property. Loading from the\n",
    "        file system relies on the provided model.name andlabel.\n",
    "        \"\"\"\n",
    "        if self.extract:\n",
    "            return self.extract_save_descriptors(dataloader, model, label)\n",
    "\n",
    "        return self.load_descriptors(model.name, label)\n",
    "\n",
    "\n",
    "descriptor_manager = DescriptorManager(extract=extract_model_descriptors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d79ee2b-33b2-4974-ac50-950fe278574c",
   "metadata": {},
   "source": [
    "### Perform k-nearest neighbors (kNN) classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655585f1-8758-481c-bc9f-2160c52df57a",
   "metadata": {},
   "source": [
    "First, we define the number of neighbors using which we would like to perform classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1583e334-83bc-4493-914e-1f22a32149d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neighbors_list = [2, 3, 5, 8, 13]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95182658-afa8-4e33-b355-4148bafd7f80",
   "metadata": {},
   "source": [
    "For each `n_neighbors` in the defined list, we perform classification and calculate the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082cba94-69c5-4e0c-860f-d1423811b954",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DescriptorAssessor:\n",
    "    def __init__(self, model, descriptor_manager):\n",
    "        self.model = model\n",
    "        self.descriptor_manager = descriptor_manager\n",
    "\n",
    "    def print_accuracies(self, accuracies, label):\n",
    "        for n_neighbors, accuracy in accuracies.items():\n",
    "            print(f\"{self.model.name} accuracy on {label} set for {n_neighbors}-neighbor classification: {accuracy}\")\n",
    "\n",
    "    def plot_accuracies_line(self, ax, accuracies, label):\n",
    "        return ax.plot(\n",
    "            accuracies.keys(),\n",
    "            list(accuracies.values()),\n",
    "            label=label,\n",
    "            marker=\"o\"\n",
    "        )\n",
    "\n",
    "    def _get_descriptors(self, dataset, data_types):\n",
    "        descriptors = {}\n",
    "\n",
    "        for data_type in data_types:\n",
    "            descriptors[data_type] = self.descriptor_manager.get_descriptors(\n",
    "                dataset.dataloaders[data_type],\n",
    "                self.model,\n",
    "                data_type\n",
    "            )\n",
    "\n",
    "        return descriptors\n",
    "\n",
    "    def assess(self, dataset, n_neighbors_list, train_type, assess_types):\n",
    "        \"\"\"\n",
    "        Assess the performance of kNN-classifiers on model descriptors for\n",
    "        different numbers of neighbors.\n",
    "\n",
    "        Keyword arguments:\n",
    "        dataset -- dictionary of data types (train, test, ...) mapped to Subsets\n",
    "        n_neighbors_list -- list of numbers of neighbors to assess\n",
    "        train_type -- the data type to which the classifier should be fit\n",
    "        assess_types -- the other types on which to assess the classifier accuracy\n",
    "        \"\"\"\n",
    "        data_types = assess_types.copy()\n",
    "        data_types.insert(0, train_type)\n",
    "\n",
    "        descriptors = self._get_descriptors(\n",
    "            dataset,\n",
    "            data_types\n",
    "        )\n",
    "\n",
    "        data_types = assess_types.copy()\n",
    "        data_types.insert(0, train_type)\n",
    "\n",
    "        predictions = dict([(data_type, {}) for data_type in data_types])\n",
    "        accuracies = dict([(data_type, {}) for data_type in data_types])\n",
    "\n",
    "        for n_neighbors in n_neighbors_list:\n",
    "            classifier = KNeighborsClassifier(n_neighbors=n_neighbors).fit(\n",
    "                descriptors[train_type],\n",
    "                dataset.targets[train_type]\n",
    "            )\n",
    "\n",
    "            for data_type in data_types:\n",
    "                predictions[data_type][n_neighbors] = classifier.predict(\n",
    "                    descriptors[data_type]\n",
    "                )\n",
    "                accuracies[data_type][n_neighbors] = accuracy_score(\n",
    "                    dataset.targets[data_type],\n",
    "                    predictions[data_type][n_neighbors]\n",
    "                )\n",
    "\n",
    "        return (descriptors, predictions, accuracies)\n",
    "\n",
    "    def visualise(self, predictions, accuracies):\n",
    "        fig, ax = plt.subplots()\n",
    "        for data_type, data_type_accuracies in accuracies.items():\n",
    "            self.print_accuracies(data_type_accuracies, data_type)\n",
    "            self.plot_accuracies_line(ax, data_type_accuracies, data_type)\n",
    "\n",
    "        ax.legend()\n",
    "        ax.set_title(f\"kNN-classification performance using {self.model.name} descriptors\")\n",
    "        ax.set_xlabel(\"k (number of neighbours)\")\n",
    "        ax.set_ylabel(\"accuracy\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5de2a8e-5e25-45c9-ae86-77955ce59457",
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptor_assessor = DescriptorAssessor(vgg16, descriptor_manager)\n",
    "descriptors, predictions, accuracies = descriptor_assessor.assess(dataset, n_neighbors_list, DataType.TRAIN, [DataType.TEST])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7630d09d-e95d-48dc-a7c2-961dbc1eb44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptor_assessor.visualise(predictions, accuracies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "babcf082-8de1-4455-8ea8-c6efe54c7f82",
   "metadata": {},
   "source": [
    "### Visually assess predicted vs. target through PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9cabfd-b6a9-4c25-b2a2-5909fa9e3bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_n_train = max(accuracies[DataType.TRAIN], key=accuracies[DataType.TRAIN].get)\n",
    "best_n_test = max(accuracies[DataType.TEST], key=accuracies[DataType.TEST].get)\n",
    "print(best_n_train)\n",
    "print(best_n_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9221e5-6d6f-4e89-8906-5ac61f8bdefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_train = PCA(n_components=best_n_train).fit_transform(descriptors[DataType.TRAIN])\n",
    "pca_test = PCA(n_components=best_n_test).fit_transform(descriptors[DataType.TEST])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ce636b-2391-4089-894f-fefc16d53430",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions_against_targets(data,\n",
    "                                     predictions,\n",
    "                                     targets,\n",
    "                                     n_classes,\n",
    "                                     cmap=None):\n",
    "    \"\"\"Draws a scatter plot of two-dimensional data which highlights\n",
    "    differences between targets and predictions through distinct edge and fill\n",
    "    colors.\"\"\"\n",
    "\n",
    "    if cmap is None:\n",
    "        cmap = plt.get_cmap(\"rainbow\")\n",
    "\n",
    "    if isinstance(targets, list):\n",
    "        targets = np.array(targets)\n",
    "\n",
    "    colors_target = cmap(targets / n_classes)\n",
    "    colors_predictions = cmap(predictions / n_classes)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    # Plot targets (ground truth) with fill color representing target class\n",
    "    ax.scatter(data[:, 0],\n",
    "               data[:, 1],\n",
    "               facecolors=colors_target,\n",
    "               label=\"target\")\n",
    "\n",
    "    # Plot predictions with edge color representing predicted class\n",
    "    ax.scatter(data[:, 0],\n",
    "               data[:, 1],\n",
    "               facecolors=\"none\",\n",
    "               edgecolors=colors_predictions,\n",
    "               label=\"predicted\")\n",
    "\n",
    "    legend = ax.legend()\n",
    "    # Set colors to black to indicate the legend is about marker type, rather\n",
    "    # than color\n",
    "    legend.legendHandles[0].set_color(\"black\")\n",
    "    legend.legendHandles[1].set_edgecolor(\"black\")\n",
    "\n",
    "    return (fig, ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3ca379-ae94-490e-b441-2ad91369dd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax_train_scatter = plot_predictions_against_targets(pca_train,\n",
    "                                                       predictions[DataType.TRAIN][best_n_train],\n",
    "                                                       dataset.targets[DataType.TRAIN],\n",
    "                                                       len(dataset.all_classes))\n",
    "ax_train_scatter.set_title(\"Two-component PCA, training data\")\n",
    "\n",
    "_, ax_test_scatter = plot_predictions_against_targets(pca_test,\n",
    "                                                      predictions[DataType.TEST][best_n_test],\n",
    "                                                      dataset.targets[DataType.TEST],\n",
    "                                                      len(dataset.all_classes))\n",
    "ax_test_scatter.set_title(\"Two-component PCA, test data\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS-CIRI-conda",
   "language": "python",
   "name": "ds-ciri-conda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
